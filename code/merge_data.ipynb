{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from phonecodes import phonecodes\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textgrid\n",
    "from scipy.spatial.distance import euclidean\n",
    "import copy\n",
    "\n",
    "import jiwer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "import soundfile as sf\n",
    "\n",
    "_ESPEAK_LIBRARY = r\"C:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n",
    "processor_P = AutoProcessor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model_P = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m similarity_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOct12\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msimilarities_with_w2vacc.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilarity_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\pandas\\io\\excel\\_base.py:465\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m     )\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmangle_dupe_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmangle_dupe_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1458\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1426\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1446\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1449\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m        DataFrame from the passed in Excel file.\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m   1459\u001b[0m         sheet_name\u001b[38;5;241m=\u001b[39msheet_name,\n\u001b[0;32m   1460\u001b[0m         header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   1461\u001b[0m         names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m   1462\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   1463\u001b[0m         usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m   1464\u001b[0m         squeeze\u001b[38;5;241m=\u001b[39msqueeze,\n\u001b[0;32m   1465\u001b[0m         converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[0;32m   1466\u001b[0m         true_values\u001b[38;5;241m=\u001b[39mtrue_values,\n\u001b[0;32m   1467\u001b[0m         false_values\u001b[38;5;241m=\u001b[39mfalse_values,\n\u001b[0;32m   1468\u001b[0m         skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[0;32m   1469\u001b[0m         nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[0;32m   1470\u001b[0m         na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[0;32m   1471\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m   1472\u001b[0m         date_parser\u001b[38;5;241m=\u001b[39mdate_parser,\n\u001b[0;32m   1473\u001b[0m         thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[0;32m   1474\u001b[0m         comment\u001b[38;5;241m=\u001b[39mcomment,\n\u001b[0;32m   1475\u001b[0m         skipfooter\u001b[38;5;241m=\u001b[39mskipfooter,\n\u001b[0;32m   1476\u001b[0m         convert_float\u001b[38;5;241m=\u001b[39mconvert_float,\n\u001b[0;32m   1477\u001b[0m         mangle_dupe_cols\u001b[38;5;241m=\u001b[39mmangle_dupe_cols,\n\u001b[0;32m   1478\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1479\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\pandas\\io\\excel\\_base.py:638\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume an integer if not a string\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[1;32m--> 638\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_float\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[0;32m    641\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:575\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[1;34m(self, sheet, convert_float)\u001b[0m\n\u001b[0;32m    573\u001b[0m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    574\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_number, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39mrows):\n\u001b[0;32m    576\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_cell(cell, convert_float) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m converted_row \u001b[38;5;129;01mand\u001b[39;00m converted_row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;66;03m# trim trailing empty elements\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[1;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_source() \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[0;32m     78\u001b[0m     parser \u001b[38;5;241m=\u001b[39m WorkSheetParser(src,\n\u001b[0;32m     79\u001b[0m                              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_strings,\n\u001b[0;32m     80\u001b[0m                              data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mdata_only,\n\u001b[0;32m     81\u001b[0m                              epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mepoch,\n\u001b[0;32m     82\u001b[0m                              date_formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39m_date_formats,\n\u001b[0;32m     83\u001b[0m                              timedelta_formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39m_timedelta_formats)\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m max_row:\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:156\u001b[0m, in \u001b[0;36mWorkSheetParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m properties \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    138\u001b[0m     PRINT_TAG: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprint_options\u001b[39m\u001b[38;5;124m'\u001b[39m, PrintOptions),\n\u001b[0;32m    139\u001b[0m     MARGINS_TAG: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_margins\u001b[39m\u001b[38;5;124m'\u001b[39m, PageMargins),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m }\n\u001b[0;32m    154\u001b[0m it \u001b[38;5;241m=\u001b[39m iterparse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource) \u001b[38;5;66;03m# add a finaliser to close the source when this becomes possible\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, element \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    157\u001b[0m     tag_name \u001b[38;5;241m=\u001b[39m element\u001b[38;5;241m.\u001b[39mtag\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag_name \u001b[38;5;129;01min\u001b[39;00m dispatcher:\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\xml\\etree\\ElementTree.py:1255\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[1;32m-> 1255\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\zipfile.py:924\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[1;32m--> 924\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\zipfile.py:992\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    990\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m--> 992\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read2(n)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\zipfile.py:1024\u001b[0m, in \u001b[0;36mZipExtFile._read2\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1021\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m   1022\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left)\n\u001b[1;32m-> 1024\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\zipfile.py:744\u001b[0m, in \u001b[0;36m_SharedFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt read from the ZIP file while there \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    741\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis an open writing handle on it. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    742\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose the writing handle before trying to read.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos)\n\u001b[1;32m--> 744\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "similarity_path=r\"..\\Oct12\\similarities_with_w2vacc.xlsx\"\n",
    "similarity = pd.read_excel(similarity_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29792615724519933"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "for _ in range(len(similarity[\"x\"])):\n",
    "    count+=jiwer.wer(similarity[\"x\"][_],similarity[\"y\"][_])\n",
    "count/len(similarity[\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_result_path=r\"..\\data\\test.xlsx\"\n",
    "human_result = pd.read_excel(human_result_path)\n",
    "human_result_1a=human_result[human_result[\"Experiment\"]==\"1a\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arpabet_to_ipa = {\n",
    "    'W': 'w',\n",
    "    'UW0': 'u',\n",
    "    'N': 'n',\n",
    "    'AW1': 'aʊ',\n",
    "    'OW0': 'oʊ',\n",
    "    'IY0': 'i',\n",
    "    'L': 'l',\n",
    "    'IY1': 'i',\n",
    "    'SH': 'ʃ',\n",
    "    'AE2': 'æ',\n",
    "    'AO1': 'ɒ',\n",
    "    'HH': 'h',\n",
    "    'V': 'v',\n",
    "    'AA2': 'ɑ',\n",
    "    'EY2': 'eɪ',\n",
    "    'AE1': 'æ',\n",
    "    'Z': 'z',\n",
    "    'UW2': 'u',\n",
    "    'D': 'd',\n",
    "    'AH2': 'ʌ',\n",
    "    'M': 'm',\n",
    "    'B': 'b',\n",
    "    'IY2': 'i',\n",
    "    'OY1': 'ɔɪ',\n",
    "    'F': 'f',\n",
    "    'CH': 'tʃ',\n",
    "    'Y': 'j',\n",
    "    'TH': 'θ',\n",
    "    'ER1': 'ɜ',\n",
    "    'ER0': 'ɜ',\n",
    "    'AO2': 'ɒ',\n",
    "    'JH': 'dʒ',\n",
    "    'UW1': 'u',\n",
    "    'P': 'p',\n",
    "    'AY1': 'aɪ',\n",
    "    'IH2': 'ɪ',\n",
    "    'T': 't',\n",
    "    'K': 'k',\n",
    "    'AO0': 'ɒ',\n",
    "    'DH': 'ð',\n",
    "    'OW2': 'oʊ',\n",
    "    'EH1': 'e',\n",
    "    'G': 'ɡ',\n",
    "    'IH0': 'ɪ',\n",
    "    'AH1': 'ʌ',\n",
    "    'EY1': 'eɪ',\n",
    "    'AH0': 'ʌ',\n",
    "    'NG': 'ŋ',\n",
    "    'AA1': 'ɑ',\n",
    "    'IH1': 'ɪ',\n",
    "    'S': 's',\n",
    "    'OW1': 'oʊ',\n",
    "    'UH1': 'ʊ',\n",
    "    'R': 'ɹ'\n",
    "}\n",
    "\n",
    "def get_pathset(paths):\n",
    "    return [os.path.join(dir, each_file) for dir, mid, files in os.walk(paths) for each_file in files if each_file.endswith(\".wav\")]\n",
    "\n",
    "def CTC_index(processor,outind):\n",
    "    meaningful_ids = []\n",
    "    meaningful_indices = []\n",
    "    previous_id = -1  \n",
    "    blank_token_id = processor.tokenizer.pad_token_id  \n",
    "    for i, token_id in enumerate(outind[0]):  \n",
    "        if token_id != previous_id and token_id != blank_token_id:\n",
    "            meaningful_ids.append(token_id.item())  \n",
    "            meaningful_indices.append(i)  \n",
    "        previous_id = token_id\n",
    "    \n",
    "    return meaningful_indices\n",
    "\n",
    "\"\"\"def get_set_diphone(paths,model,processor):\n",
    "    out_dict={}\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    #english_phoneme_dict.values()\n",
    "    for each_sentence in tqdm.tqdm(paths):\n",
    "        tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "        tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "        wave, sr = librosa.load(each_sentence)\n",
    "        wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "        #wave_res = wave_res[:int(sentence16_end_time*16000)]\n",
    "        for each_tg in tg_sentence:\n",
    "            '''start=round(each_tg.minTime*16000)\n",
    "            end=round(each_tg.maxTime*16000)\n",
    "            input=processor(wave_res[start:end],sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "            input=input.to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder1=model(input).logits\n",
    "            selected=out_encoder1\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "            phonemeindex = CTC_index(processor,outind)\n",
    "            out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()'''\n",
    "            \n",
    "            tg_word = [i for i in tg[1] if each_tg.minTime<=i.minTime and each_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "            for each_word_tg in tg_word:\n",
    "                each_word_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if each_word_tg.minTime<=i.minTime and each_word_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "                sentence_total_length=each_tg.maxTime-each_tg.minTime\n",
    "                word_cut_start=each_word_tg.minTime-each_tg.minTime\n",
    "                word_cut_end=each_word_tg.maxTime-each_tg.minTime\n",
    "                input=processor(wave_res[int(each_tg.minTime*16000):round(each_tg.maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "                model.to(device)\n",
    "                with torch.no_grad():\n",
    "                    out_encoder=model(input).logits\n",
    "                    \n",
    "                word_start=round(out_encoder.shape[1]*word_cut_start/sentence_total_length)\n",
    "                word_end=round(out_encoder.shape[1]*word_cut_end/sentence_total_length)\n",
    "                selected=out_encoder[:,word_start:word_end,:]\n",
    "                mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "                mask[list(english_phoneme_dict.values())] = False\n",
    "                selected[:, :, mask] = 0\n",
    "                outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "                phonemeindex = CTC_index(processor,outind)\n",
    "                transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "                #aligned_seq1, aligned_seq2 ,phonemeindex= align_sequences_with_index(each_word_phonemes,transcription,phonemeindex)\n",
    "                aligned_seq1, aligned_seq2 = align_sequences(each_word_phonemes,transcription)\n",
    "                out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()\n",
    "                if not len(aligned_seq1)==len(aligned_seq2)==len(phonemeindex):\n",
    "                    print(len(aligned_seq1),len(aligned_seq2),len(phonemeindex))\n",
    "                    print(aligned_seq1,aligned_seq2,phonemeindex)\n",
    "                    raise IndexError(\"length unmatch\")\n",
    "                for i in range(len(aligned_seq1)-1):\n",
    "                    key = aligned_seq1[i] + aligned_seq1[i + 1]\n",
    "                    if key not in out_dict:\n",
    "                        out_dict[key] = []\n",
    "                    out_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))\n",
    "            torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    return out_dict\"\"\"\n",
    "\n",
    "def get_set_diphone(paths,model,processor):\n",
    "    out_dict={}\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    #english_phoneme_dict.values()\n",
    "    for each_sentence in tqdm.tqdm(paths):\n",
    "        tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "        tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "        wave, sr = librosa.load(each_sentence)\n",
    "        wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "        #wave_res = wave_res[:int(sentence16_end_time*16000)]\n",
    "        for each_tg in tg_sentence:\n",
    "            each_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if each_tg.minTime<=i.minTime and each_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "            start=int(each_tg.minTime*16000)\n",
    "            end=round(each_tg.maxTime*16000)\n",
    "            input=processor(wave_res[start:end],sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "            input=input.to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder1=model(input).logits\n",
    "            selected=out_encoder1\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "            phonemeindex = CTC_index(processor,outind)\n",
    "            aligned_seq1, aligned_seq2 ,phonemeindex= align_sequences_with_index(each_phonemes,transcription,phonemeindex)\n",
    "            if phonemeindex[0]>phonemeindex[1]:\n",
    "                phonemeindex[0]=0\n",
    "            elif phonemeindex[0]==phonemeindex[1] and phonemeindex[1]>phonemeindex[2]:\n",
    "                phonemeindex[0],phonemeindex[1]=0,1\n",
    "            #aligned_seq1, aligned_seq2 = align_sequences(each_phonemes,transcription)\n",
    "            if not len(aligned_seq1)==len(aligned_seq2)==len(phonemeindex):\n",
    "                print(len(aligned_seq1),len(aligned_seq2),len(phonemeindex))\n",
    "                print(aligned_seq1,\"\\n\",aligned_seq2,\"\\n\",phonemeindex)\n",
    "                raise IndexError(\"length unmatch\")\n",
    "            \n",
    "            out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()\n",
    "            for i in range(len(transcription)-1):#aligned_seq1\n",
    "                #key = aligned_seq1[i] + aligned_seq1[i + 1]\n",
    "                key = transcription[i] + transcription[i + 1]\n",
    "                if key not in out_dict:\n",
    "                    out_dict[key] = []\n",
    "                try:\n",
    "                    out_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))\n",
    "                except:\n",
    "                    #print(each_tg)\n",
    "                    #print(aligned_seq1)\n",
    "                    #print(aligned_seq2)\n",
    "                    #print(transcription)\n",
    "                    #print(phonemeindex)\n",
    "                    #print(key)\n",
    "                    out_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i]])))\n",
    "            torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    return out_dict\n",
    "\n",
    "def get_training_paths(TrainingTalkerID,all_path):\n",
    "    path_list=[]\n",
    "    TalkerID=[]\n",
    "    for each_ID in TrainingTalkerID.split(\", \"):\n",
    "        if each_ID[:3]==\"CMN\":\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_CMN_ENG_HT1\")\n",
    "        else:\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_ENG_ENG_HT1\")\n",
    "    \n",
    "    for each_path in TalkerID:\n",
    "        for i in all_path:\n",
    "            if each_path in i:\n",
    "                path_list.append(i)\n",
    "                break\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "def align_sequences(seq1, seq2):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,   \n",
    "                           dp[i][j - 1] + 1,   \n",
    "                           dp[i - 1][j - 1] + cost)       \n",
    "    aligned_seq1, aligned_seq2 = [], []\n",
    "    i, j = len1, len2\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and dp[i][j] == dp[i - 1][j] + 1:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append('<pad>')  \n",
    "            i -= 1\n",
    "        elif j > 0 and dp[i][j] == dp[i][j - 1] + 1:\n",
    "            aligned_seq1.append('<pad>')\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            j -= 1\n",
    "        else:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    return aligned_seq1[::-1], aligned_seq2[::-1]\n",
    "\n",
    "def align_sequences_with_index(seq1, seq2, index_list):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "\n",
    "\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,   \n",
    "                           dp[i][j - 1] + 1,   \n",
    "                           dp[i - 1][j - 1] + cost)\n",
    "\n",
    "\n",
    "    aligned_seq1, aligned_seq2, aligned_index_list = [], [], []\n",
    "    i, j = len1, len2\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and dp[i][j] == dp[i - 1][j] + 1:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append('<pad>')\n",
    "            aligned_index_list.append(index_list[j - 1] + 1) \n",
    "            i -= 1\n",
    "        elif j > 0 and dp[i][j] == dp[i][j - 1] + 1:\n",
    "            aligned_seq1.append('<pad>')\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            aligned_index_list.append(index_list[j - 1]) \n",
    "            j -= 1\n",
    "        else:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            aligned_index_list.append(index_list[j - 1])  \n",
    "            i -= 1\n",
    "            j -= 1\n",
    "\n",
    "\n",
    "    return aligned_seq1[::-1], aligned_seq2[::-1], aligned_index_list[::-1]\n",
    "\n",
    "def build_exposure_set(paths, native_dict, set_list, model,processor):\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    english_phoneme_dict.values()\n",
    "    for each_sentence in paths:\n",
    "        tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "        tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "        tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "        tg_sentence = [each for _,each in enumerate(tg_sentence) if _ in set_list]\n",
    "        '''sentence16_end_time=tg_sentence[15].maxTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.maxTime<=sentence16_end_time]\n",
    "        tg_word = [i for i in tg_word if i.maxTime<=sentence16_end_time]'''\n",
    "        \n",
    "        wave, sr = librosa.load(each_sentence)\n",
    "        wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "        #wave_res = wave_res[:int(sentence16_end_time*16000)]\n",
    "        for each_tg in tg_sentence:\n",
    "            start=round(each_tg.minTime*16000)\n",
    "            end=round(each_tg.maxTime*16000)\n",
    "            input=processor(wave_res[start:end],sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "            input=input.to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder1=model(input).logits\n",
    "            selected=out_encoder1\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            #outind=torch.argmax(out_encoder1,dim=-1).cpu().numpy()\n",
    "            transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "            phonemeindex = CTC_index(processor,outind)\n",
    "            out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()\n",
    "            for i in range(len(transcription)-1):\n",
    "                key = transcription[i] + transcription[i + 1]\n",
    "                if key not in native_dict:\n",
    "                    native_dict[key] = []\n",
    "                native_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    return native_dict\n",
    "    #'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\ALL_032_M_CMN_ENG_HT1.wav'\n",
    "    \n",
    "    \n",
    "def get_test_list(file_path,key_word,sentenceID,model,processor):\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    sentenceID=int(sentenceID[-3:])-1\n",
    "    #file_path= f'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\{file_path[:-5]}.wav'\n",
    "    \n",
    "    tg = textgrid.TextGrid.fromFile(file_path[:-3]+\"TextGrid\")\n",
    "    tg_sentence = [i for i in tg[0] if i.mark!=\"\"][sentenceID]\n",
    "    \n",
    "    tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "    \n",
    "    wave, sr = librosa.load(file_path)\n",
    "    wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "    \n",
    "\n",
    "    for each_word_tg in tg_word:\n",
    "        if each_word_tg.minTime >= tg_sentence.minTime and each_word_tg.maxTime <= tg_sentence.maxTime:\n",
    "            #print(each_word_tg.mark.lower(),key_word)\n",
    "            if each_word_tg.mark.lower()==key_word:\n",
    "                start=each_word_tg.minTime\n",
    "                end=each_word_tg.maxTime\n",
    "                break\n",
    "                #print(\"start:\",start,\"end:\",end)\n",
    "    #word_length=len(wave_res)/16000\n",
    "    out_list=[]\n",
    "    each_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if each_word_tg.minTime<=i.minTime and each_word_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "    sentence_total_length=tg_sentence.maxTime-tg_sentence.minTime\n",
    "    word_cut_start=start-tg_sentence.minTime\n",
    "    word_cut_end=end-tg_sentence.minTime\n",
    "    \n",
    "    input=processor(wave_res[int(tg_sentence.minTime*16000):round(tg_sentence.maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_encoder=model(input.to(device)).logits\n",
    "        out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().numpy()\n",
    "    \n",
    "    word_start=round(out_encoder.shape[1]*word_cut_start/sentence_total_length)\n",
    "    word_end=round(out_encoder.shape[1]*word_cut_end/sentence_total_length)\n",
    "    \n",
    "    selected=out_encoder[:,word_start:word_end,:]\n",
    "    mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "    mask[list(english_phoneme_dict.values())] = False\n",
    "    selected[:, :, mask] = 0\n",
    "    outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "    phonemeindex = CTC_index(processor,outind)\n",
    "    transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "    aligned_seq1, aligned_seq2 ,phonemeindex_= align_sequences_with_index(each_phonemes,transcription,phonemeindex)\n",
    "    \n",
    "            \n",
    "    if len(phonemeindex)<2:\n",
    "        each_FE = out_FE[word_start:,:]\n",
    "        selected=out_encoder[:,word_start:,:]\n",
    "        mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "        mask[list(english_phoneme_dict.values())] = False\n",
    "        selected[:, :, mask] = 0\n",
    "        outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "        phonemeindex = CTC_index(processor,outind)\n",
    "        transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "        \n",
    "        diphone_key = transcription[0] + transcription[0 + 1]\n",
    "        out_list.append((diphone_key, np.vstack((each_FE[phonemeindex[0]], each_FE[phonemeindex[0 + 1]]))))\n",
    "\n",
    "    else:\n",
    "        if phonemeindex_[0]>phonemeindex[1]:\n",
    "            phonemeindex_[0]=0\n",
    "        elif phonemeindex_[0]==phonemeindex_[1] and phonemeindex_[1]>phonemeindex_[2]:\n",
    "            phonemeindex_[0],phonemeindex_[1]=0,1\n",
    "            \n",
    "        if not len(aligned_seq1)==len(aligned_seq2)==len(phonemeindex_):\n",
    "            print(len(aligned_seq1),len(aligned_seq2),len(phonemeindex_))\n",
    "            print(aligned_seq1,\"\\n\",aligned_seq2,\"\\n\",phonemeindex_)\n",
    "            raise IndexError(\"length unmatch\")\n",
    "        each_FE = out_FE[word_start:word_end,:]\n",
    "        for i in range(len(aligned_seq1)-1):\n",
    "            diphone_key = aligned_seq1[i] + aligned_seq1[i + 1]\n",
    "            try:\n",
    "                out_list.append((diphone_key, np.vstack((each_FE[phonemeindex_[i]], each_FE[phonemeindex_[i + 1]]))))\n",
    "            except:\n",
    "                out_list.append((diphone_key, np.vstack((each_FE[phonemeindex_[i]], each_FE[phonemeindex_[i]]))))\n",
    "    torch.cuda.empty_cache()\n",
    "    return out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['h', 'ʌ', 'z', 'b', 'ʌ', 'n', 'd'],\n",
       " ['h', 'ʌ', 'z', 'b', 'ɪ', 'n', '<pad>'],\n",
       " [1, 4, 9, 13, 15, 20, 21])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_sequences_with_index(['h', 'ʌ', 'z', 'b', 'ʌ', 'n', 'd'],['h', 'ʌ', 'z', 'b', 'ɪ', 'n'],[1, 4, 9, 13, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Interval(15.96449, 17.54544, SHE LOST HER CREDIT CARD)\n",
      "19 Interval(37.4518, 39.05175, A LETTER FELL ON THE FLOOR)\n",
      "23 Interval(45.12955, 46.6985, SHE STOOD NEAR THE WINDOW)\n",
      "38 Interval(74.7756, 76.49455, THEY WANTED SOME POTATOES)\n",
      "43 Interval(84.78528, 86.24623, SUGAR IS VERY SWEET)\n",
      "50 Interval(97.40487, 99.05382, THE WAITER BROUGHT THE CREAM)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tg = textgrid.TextGrid.fromFile(ALL_ENG_ENG_pathset[33][:-3]+\"TextGrid\")\n",
    "tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "for _,each_tg in enumerate(tg_sentence):\n",
    "    try:\n",
    "        each_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if each_tg.minTime<=i.minTime and each_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "    except:\n",
    "        print(_,each_tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:06<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "ALL_ENG_ENG_path=r\"..\\data\\raw_L1\"\n",
    "ALL_ENG_ENG_pathset=get_pathset(ALL_ENG_ENG_path)\n",
    "ALL_ENG_ENG_dict = get_set_diphone(ALL_ENG_ENG_pathset, model_P, processor_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_measure(df,all_eng_dict, model, processor):\n",
    "    sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count =[], [], [], [], []\n",
    "    confusion_matrix_list, wav2vec_acc_list=[],[]\n",
    "    X_list=[]\n",
    "    Y_list=[]\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    \n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    \n",
    "    phoneme_vocab = {phoneme: idx for idx, phoneme in enumerate(english_phonemes)}\n",
    "    train_set_dict={}\n",
    "    test_word_dict={}\n",
    "    test_matrix_dict={}\n",
    "    for each_ in tqdm.tqdm(df.values):\n",
    "        filename_loc=df.columns.get_loc(\"Filename\")\n",
    "        keyword_loc=df.columns.get_loc(\"Keyword\")\n",
    "        training_talker_loc=df.columns.get_loc(\"TrainingTalkerID\")\n",
    "        \n",
    "        all_path=get_pathset(r\"..\\data\\raw\")\n",
    "        all_ENG_ENG_pathset=[s.replace(\"raw_L1\", \"raw\") for s in get_pathset(r\"..\\data\\raw_L1\")]\n",
    "        \n",
    "        set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "        set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "        if each_[df.columns.get_loc(\"TrainingTestSet\")] == \"set2,set1\":\n",
    "            train_set=set2_list\n",
    "            test_set=set1_list\n",
    "        else:\n",
    "            train_set=set1_list\n",
    "            test_set=set2_list\n",
    "        \n",
    "        #print(each_[filename_loc])\n",
    "        test_file = [each for each in all_path if os.path.split(each_[filename_loc])[-1][:-5] in each]\n",
    "        #print(test_file)\n",
    "        key_word = each_[keyword_loc] #string\n",
    "        TrainingTalkerID = each_[training_talker_loc] #list of string\n",
    "        sentenceID = each_[df.columns.get_loc(\"SentenceID\")]\n",
    "        training_files_path=get_training_paths(TrainingTalkerID,all_path)\n",
    "        \n",
    "        if training_files_path[0] in all_ENG_ENG_pathset:\n",
    "            training_dict=copy.deepcopy(all_eng_dict)\n",
    "        else:\n",
    "            if TrainingTalkerID not in train_set_dict:\n",
    "                train_set_dict[TrainingTalkerID]={}\n",
    "                \n",
    "            if each_[df.columns.get_loc(\"TrainingTestSet\")] not in train_set_dict[TrainingTalkerID]:\n",
    "                training_dict=build_exposure_set(training_files_path, copy.deepcopy(all_eng_dict), train_set, model, processor)\n",
    "                train_set_dict[TrainingTalkerID][each_[df.columns.get_loc(\"TrainingTestSet\")]]=copy.deepcopy(training_dict)\n",
    "            else:\n",
    "                training_dict=train_set_dict[TrainingTalkerID][each_[df.columns.get_loc(\"TrainingTestSet\")]]\n",
    "\n",
    "        \n",
    "        \n",
    "        if test_file[0] not in test_word_dict:\n",
    "            test_word_dict[test_file[0]]={}\n",
    "        if sentenceID not in test_word_dict[test_file[0]]:\n",
    "            test_word_dict[test_file[0]][sentenceID]={}\n",
    "        if key_word not in test_word_dict[test_file[0]][sentenceID]:\n",
    "            test_list = get_test_list(test_file[0], key_word, sentenceID, model, processor)\n",
    "            test_word_dict[test_file[0]][sentenceID][key_word]=copy.deepcopy(test_list)\n",
    "        else:\n",
    "            test_list=test_word_dict[test_file[0]][sentenceID][key_word]\n",
    "        \n",
    "        if test_file[0] not in test_matrix_dict:\n",
    "            test_matrix_dict[test_file[0]]={}\n",
    "        if sentenceID not in test_matrix_dict[test_file[0]]:\n",
    "            test_matrix_dict[test_file[0]][sentenceID]={}\n",
    "        if key_word not in test_matrix_dict[test_file[0]][sentenceID]:\n",
    "            \n",
    "            tg = textgrid.TextGrid.fromFile(test_file[0][:-3]+\"TextGrid\")\n",
    "            tg_sentence = [i for i in tg[0] if i.mark!=\"\"][int(sentenceID[-3:])-1]\n",
    "            tg_word = [i for i in tg[1] if tg_sentence.minTime<=i.minTime and tg_sentence.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark.lower()==key_word][0]\n",
    "            \n",
    "            each_word_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if tg_word.minTime<=i.minTime and tg_word.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "            \n",
    "            sentence_total_length=tg_sentence.maxTime-tg_sentence.minTime\n",
    "            word_cut_start=tg_word.minTime-tg_sentence.minTime\n",
    "            word_cut_end=tg_word.maxTime-tg_sentence.minTime\n",
    "            wave, sr = librosa.load(test_file[0])\n",
    "            wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "            input=processor(wave_res[int(tg_sentence.minTime*16000):round(tg_sentence.maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder=model(input.to(device)).logits\n",
    "                \n",
    "            word_start=round(out_encoder.shape[1]*word_cut_start/sentence_total_length)\n",
    "            word_end=round(out_encoder.shape[1]*word_cut_end/sentence_total_length)\n",
    "            selected=out_encoder[:,word_start:word_end,:]\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            #phonemeindex = CTC_index(processor,outind)\n",
    "            transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "            X_=each_word_phonemes\n",
    "            Y_=transcription\n",
    "            #print(each_word_phonemes,\"\\n\",transcription)\n",
    "            aligned_seq1, aligned_seq2 = align_sequences(each_word_phonemes,transcription)\n",
    "            \n",
    "            N=len(list(phoneme_vocab.keys()))\n",
    "            confusion_matrix = np.zeros((N, N), dtype=int)\n",
    "            #print(confusion_matrix.shape)\n",
    "            for true_phoneme, predicted_phoneme in zip(aligned_seq1, aligned_seq2):\n",
    "                if predicted_phoneme in english_phonemes[:4]:\n",
    "                    true_idx = phoneme_vocab[true_phoneme]\n",
    "                    predicted_idx = phoneme_vocab[true_phoneme]\n",
    "                else:\n",
    "                    true_idx = phoneme_vocab[true_phoneme]\n",
    "                    predicted_idx = phoneme_vocab[predicted_phoneme]\n",
    "                #print(true_idx, predicted_idx)\n",
    "                confusion_matrix[predicted_idx,true_idx] += 1\n",
    "            \n",
    "            \n",
    "            phoneme_error = [1 if aligned_seq1[_]==aligned_seq2[_] else 0 for _ in range(len(aligned_seq1))]\n",
    "            #print(np.array(list(english_phoneme_dict.values())))\n",
    "            #confusion_matrix = #out_encoder[:,word_start:word_end,list(english_phoneme_dict.values())].cpu().numpy()\n",
    "            test_matrix_dict[test_file[0]][sentenceID][key_word]=(X_,Y_,phoneme_error, confusion_matrix)\n",
    "        else:\n",
    "            X_=test_matrix_dict[test_file[0]][sentenceID][key_word][0]\n",
    "            Y_=test_matrix_dict[test_file[0]][sentenceID][key_word][1]\n",
    "            phoneme_error = test_matrix_dict[test_file[0]][sentenceID][key_word][2]\n",
    "            confusion_matrix = copy.deepcopy(test_matrix_dict[test_file[0]][sentenceID][key_word][3])\n",
    "        \n",
    "        # word level, list\n",
    "        sim_max=[]\n",
    "        sim_std=[]\n",
    "        sim_mean=[]\n",
    "        isincluded=[]\n",
    "        #sim_count=[]\n",
    "        for _, each_diphone in enumerate(test_list):\n",
    "            \n",
    "            sims=[]\n",
    "            if each_diphone[0] in training_dict.keys():\n",
    "                #if phoneme_error[_]==1 and phoneme_error[_+1]==1:\n",
    "                    #isincluded.append(1)\n",
    "                    \n",
    "                for each_vec in training_dict[each_diphone[0]]:\n",
    "                    d=euclidean(each_diphone[1].ravel(),each_vec.ravel())\n",
    "                    sim=np.exp(-0.1*d)\n",
    "                    sims.append(sim)\n",
    "                        #sims.append(0)\n",
    "                #else:\n",
    "                    #isincluded.append(1)\n",
    "                    #sims.append(0)\n",
    "            else:\n",
    "                isincluded.append(0)\n",
    "                sims.append(0)\n",
    "            #sim_count.append(len(sim))\n",
    "            sim_max.append(np.max(sims))\n",
    "            sim_std.append(np.std(sims))\n",
    "            sim_mean.append(np.mean(sims))\n",
    "            \n",
    "            \n",
    "        sim_mean_max=np.mean(sim_max)\n",
    "        sim_mean_std=np.mean(sim_std)\n",
    "        sim_mean_mean=np.mean(sim_mean)\n",
    "        \n",
    "        X_list.append(X_)\n",
    "        Y_list.append(Y_)\n",
    "        wav2vec_acc_list.append(np.count_nonzero(phoneme_error)/len(phoneme_error))\n",
    "        confusion_matrix_list.append(confusion_matrix)\n",
    "        sim_mean_max_list.append(sim_mean_max)\n",
    "        sim_mean_std_list.append(sim_mean_std)\n",
    "        sim_mean_mean_list.append(sim_mean_mean)\n",
    "        isincluded_list.append(np.count_nonzero(isincluded))\n",
    "        diphone_count.append(len(isincluded))\n",
    "        \n",
    "    return sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count,X_list,Y_list,wav2vec_acc_list, confusion_matrix_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16477 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16477/16477 [12:41<00:00, 21.64it/s] \n"
     ]
    }
   ],
   "source": [
    "sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count,X_list,Y_list,wav2vec_acc_list, confusion_matrix_list=sim_measure(human_result_1a,ALL_ENG_ENG_dict,model_P, processor_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_160232\\2755506731.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_max\"]=sim_mean_max_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_160232\\2755506731.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_std\"]=sim_mean_std_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_160232\\2755506731.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_mean\"] = sim_mean_mean_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_160232\\2755506731.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"diphone_overlapped\"]=isincluded_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_160232\\2755506731.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"NumDiphone_word\"]=diphone_count\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_160232\\2755506731.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"wav2vec_acc\"]=wav2vec_acc_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_160232\\2755506731.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"x\"]=X_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_160232\\2755506731.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"y\"]=Y_list\n"
     ]
    }
   ],
   "source": [
    "human_result_1a[\"sim_mean_max\"]=sim_mean_max_list\n",
    "human_result_1a[\"sim_mean_std\"]=sim_mean_std_list\n",
    "human_result_1a[\"sim_mean_mean\"] = sim_mean_mean_list\n",
    "human_result_1a[\"diphone_overlapped\"]=isincluded_list\n",
    "human_result_1a[\"NumDiphone_word\"]=diphone_count\n",
    "human_result_1a[\"wav2vec_acc\"]=wav2vec_acc_list\n",
    "human_result_1a[\"x\"]=X_list\n",
    "human_result_1a[\"y\"]=Y_list\n",
    "human_result_1a.to_excel('similarities_with_actual_label.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 50])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_P.wav2vec2.feature_extractor((torch.rand(1,16080)).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321.6"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16080/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "a==[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, None, 1, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j for i in [[1,None],[1],[1,2,3,4]] for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesPCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
